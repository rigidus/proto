#+STARTUP: showall indent hidestars

Существует несколько вариантов ускорить вычисления за счет низкоуровневого программирования:
- Вынести вычисление в отдельный процесс, написанный на низкоуровневом языке и обмениваться с ним данными
  - через Memory mapped files
  - через socket или pipe
  - через очередь
- Писать низкоуровневый код так чтобы он становился частью GO-бинарника.
  - На языке Си
  - На языке Си с ассемблерными вставками
  - На чистом ассемблере с подключением во время линковки
  - На языке Go-ассемблера

Первый способ выглядит более простым, но череват затратами на межпроцессную коммуникацию, поэтому я начну исследовать с более сложного второго.

Также существует проект https://github.com/minio/c2goasm который позволяет преобразовать ассемблерные .s файлы в Go-ассемблерные. Он показывает, как сишные функции с интрисинками могут быть преобразованы в Go-ассемблер.

* DONE How to get go assembly code my Go-program

Let's start with GoLang helloworld:

#+BEGIN_SRC go
  // main.go
  package main

  import (
      "fmt"
  )

  func main() {
      fmt.Println("Hello World!")
  }
#+END_SRC

Build it:

#+BEGIN_SRC sh
  # Generates obj file as main.o
  go tool compile main.go

  # Generates assembly, and sends it to a new main.asm
  go tool compile -S main.go > main.asm

  # Specials:
  GOOS=linux GOARCH=amd64 go tool compile -S main.go > main.asm
  # or:
  go build -gcflags -S main.go
#+END_SRC

Now we have main.asm with go assembly

* How to compile and call C-function from Go

Если сделать import "C", то комментарий, который идет перед ним

#+BEGIN_SRC go
  package main

  //int pt(){
  // return 1;
  //}
  import "C"
  import "fmt"

  func main() {
      fmt.Println(C.pt())
  }
#+END_SRC

build:

#+BEGIN_SRC sh
  env CGO_ENABLED=1 GOOS=linux go build test.go
  # variant with linking .o file:
  # CGO_ENABLED=1 GOOS=linux go build -buildmode=plugin -o path/to/module.so test.go
#+END_SRC

* TODO How I can see assembly code for only one my functions
* TODO How I can wrote assebly functions and call it from GO?
* TODO How I can call GO-function and use GO-data from my assembly code?

Если в пакете есть какие-либо файлы ~.s~, то ~go build~ укажет
компилятору создать специальный заголовок с именем ~go_asm.h~, который
файлы ~.s~ могут затем ~#include~.

Файл содержит символические константы #define для смещений полей
структуры Go, размеры типов структур Go и большинство объявлений
констант Go, определенных в текущем пакете. При сборке Go следует
избегать предположений о компоновке типов Go и вместо этого
использовать эти константы. Это улучшает читаемость кода сборки и
сохраняет его устойчивость к изменениям.

Константы имеют вид ~const_name~. Например, учитывая объявление

#+BEGIN_SRC go
  const bufSize = 1024
#+END_SRC

ассемблерный код может ссылаться на значение этой константы как
~const_bufSize~.

Смещения полей имеют вид ~type_field~. Размеры структуры имеют вид
type__size. Например, рассмотрим следующее определение Go:

#+BEGIN_SRC go
  type reader struct {
      buf [bufSize]byte
      r   int
  }
#+END_SRC

Ассемблер может ссылаться на размер этой структуры как на
~reader__size~, а смещения двух полей как на ~reader_buf~ и
~reader_r~. Следовательно, если регистр ~R1~ содержит указатель на
reader, ассемблер может ссылаться на поле ~r~ как ~reader_r(R1)~.

Если любое из этих #define имен неоднозначно (например, структура с
полем _size), #include "go_asm.h" завершится ошибкой с ошибкой
"redefinition of macro"

детали в Quick guide to Go's Assembler.

* Notices

  The FUNCDATA and PCDATA directives contain information for use by
  the garbage collector; they are introduced by the compiler.

* Links

  https://pkg.go.dev/cmd/cgo
  https://go.dev/doc/asm

* Configuration

config.yaml лежит в корне репозитория.

в нем есть переменная chain_id - установить в 1 чтобы смотреть на
etherium mainnet.

после этого в ./cmd/pathfinder/ сделать

#+BEGIN_SRC shell
  go run main.go
#+END_SRC

или просто

#+BEGIN_SRC shell
  go build
#+END_SRC

Создается бинарник и его можно запустить

#+BEGIN_SRC shell
  ./cmd/pathfinder/pathfinder
  INFO[0000] Viper using supplied config
  {"err":"dial tcp 127.0.0.1:6379: connect: connection refused","level":"warning","msg":"redis init fail: program will start without redis","time":"2021-10-11T14:12:44+03:00"}
  {"level":"warning","msg":"redis not available for SaveExchangesFromRedisToMemory. SaveExchangesFromRedisToMemory waiting when redis will be available","time":"2021-10-11T14:12:45+03:00"}
  after retry
  {"level":"info","msg":"running with chain id: 1","time":"2021-10-11T14:12:45+03:00"}
  ⇨ http server started on [::]:8088
#+END_SRC

Сейчас в базовом сетапе все данные запрашиваются с ноды, поэтому можно
запросить из броузера API:

#+BEGIN_SRC html
  http://localhost:8088/v1.0/quote?deepLevel=2&mainRouteParts=10&parts=50&virtualParts=50&walletAddress=null&fromTokenAddress=0xeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee&toTokenAddress=0x6b175474e89094c44da98b954eedeac495271d0f&amount=1000000000000000000&gasPrice=73000000000&protocolWhiteList=WETH,UNISWAP_V1,UNISWAP_V2,SUSHI,MOONISWAP,BALANCER,COMPOUND,CURVE,CHAI,OASIS,KYBER,AAVE,IEARN,BANCOR,PMM1,CREAMSWAP,SWERVE,BLACKHOLESWAP,DODO,DODO_V2,VALUELIQUID,SHELL,DEFISWAP,COFIX,SAKESWAP,LUASWAP,MINISWAP,MSTABLE,PMM2,AAVE_LIQUIDATOR,SYNTHETIX,AAVE_V2,ST_ETH,ONE_INCH_LP,ONE_INCH_LP_1_1,ONE_INCH_LP_MIGRATOR,ONE_INCH_LP_MIGRATOR_V1_1,UNISWAP_V2_MIGRATOR,SUSHISWAP_MIGRATOR,LINKSWAP,S_FINANCE,PSM,POWERINDEX,INDEXED_FINANCE,PMM3,XSIGMA,CREAM_LENDING,SMOOTHY_FINANCE,SADDLE,PMM4,KYBER_DMM,BALANCER_V2,UNISWAP_V3,SETH_WRAPPER,CURVE_V2,CONVERGENCE_X,ONE_INCH_LIMIT_ORDER,DFX_FINANCE,FIXED_FEE_SWAP,DXSWAP,CLIPPER,SHIBASWAP,UNIFI,PMMX,PMM5,PSM_PAX,PMM2MM
#+END_SRC

Этот урл можно выдрать из консоли разработчика прямо с фронтенда.

* Run

Когда pathfinder идет за данными он идет через проксю, у которой есть
урл, заданный в config.yaml как chain_node.

Эта прокся перебрасывает на внутренние ноды, а внутри уже есть какой-то
кластер из нод.

* Algo

Хэндлеры, которые обрабатывают запросы к API лежат в

#+BEGIN_SRC sh
  ./quotes/delivery/http/
#+END_SRC

Например, есть функция

#+BEGIN_SRC go
  func (h *Handler) Quote(c echo.Context)
#+END_SRC

В её начале идут куски от трейсера который сейчас не нужны.

После прихода запроса вызывается фрейморковская функция Bind которая
парсит все в структуру, потом эта структура отправляется в функцию
toQuoteParams:

#+BEGIN_SRC go
  request, err := h.toQuoteParams(quoteReqParams)
#+END_SRC

там происходят дополнительные проверки, маппинг и она возвращает
структуру quotesParams. Её сорцы находятся на этом же уровне в файле
models.go.

Алгоритм рассчета пути лежит в:

#+BEGIN_SRC sh
  ./application/pathfinder/
#+END_SRC

там нужна вся папка но самое интересное в файле routes.go где функция,
которая вызывается из хэндлеров:

#+BEGIN_SRC go
  func (a *Algo) FindBestRoute
#+END_SRC

Из нее вызовется функция ~ComputeBestRoutes~, которая там же ниже, где и
будет все процесситься и формироваться результат. Вокруг этой функции
конструируется замыкание (cbr), которое запускается один раз, а потом,
возможно второй. В чем разница между этими запусками? У нас есть
возможность конструировать путь исключительно через форки uniswap-v2 и
это (вторая) попытка построить более простой путь. Потом сравнивается
какой из результатов лучше - это нужно потому что алгоритм местами жадный
и сложно поддержать и тот и другой flow. В реальности unoswap может
выиграть только на маленьких amounts (до 1000 долларов), потому что на
малых суммах важность комиссии выше..

Основные конфигруационный переменные которые
передаются это:
- Parts
- MainRouteParts
Они отвечают за дробление в алгоритме. 1 - значит путь должен быть
прямым, никакого дробления.

Все источники беруться из конфига (см. Cfg.Algo.UnoSwap)

Используется прометеус и его пакет VictoriaMetrics. Счетчики апдейтся
in-memory и Prometeus пулит эти данные.

Обертки над метриками лежат в папке ./metrics

Например в algo.go есть

#+BEGIN_SRC go
  func UpdateComputeReturnsDuration
#+END_SRC

и он вызывет из VictoriaMetrics фунцию metrics.GetOrCreateSummaryExt

[TODO:gmm] - интерфейс VictoriaMetrics

toTokenPrice и toTokenData фетчат цену и decimals по адресу
токена. Конструируется denominator. Если фетчить не получилось то ставим
какой-то фикс, чтобы выдать хоть какой-то результат. Это иногда бывает с
новыми токенами.

Нам нужно получить стоимость токена в ETH чтобы понимать что лучше -
получить больше токена с большей комиссией или наоборот.

** ComputeRoutes

...считает на всех переденных ей путях лучшее возможное разбиение по
маркетам.

Параметры:
- parths - это разбиения внутри пути (они отрисовываются внутри блоков в
  интерфейсе). Пути - это пути с хопами от токена к токену (не более 3х
  хопов). По дефолту у нас есть 50 возможных разбиений (parths) путей внутри
  пути. Этот параметр регулирует это кол-во разбиений. Диапазон: 1-100
- mainRootParts - это параметр разбиения между блоками-путями (более
  общий) Диапазон: 1-20
- deepLevel - параметр отвечает за то насколько путь будет
  длинным. deeplevel=1 значит один токен между меняемымы, deeplevel=2
  встречается нечасто (WBTC-ETH-USDC-USDT) но является рекомендуемым для
  API, еще можно deeplevel=3. По deeplevel мы определяем набор
  domain.TokenPaths которые передаются как параметр в ~computeRoutes~ -
  example [[USDC-USDC-DAI], [USDT-DAI], [USDT-ETH-DAI]] - 3 пути. (В
  графане есть метрика Latency by DeepLevel + Parth чтобы наблюдать за
  этим.)
- gasPrice - каждый маркет требует сколько-то газа, этот параметр нужен
  чтобы понять реальную комиссию за использование этого маркета.
- availableMarkets - массив массивов в каждом из эл-тов которого лежат
  все допустимые маркеты для каждой из пар параметра path. Они между
  собой могут пересекатся, поэтому там есть нюансы: ~filterOutUsedMarkets~
  чтобы не использовать повторяющиеся маркеты

*** a.ComputeDistibution

tokenService кэшируют нам данные по маркетам.

CalculateMarketReturns
.->getReturns - общая функция считающаяся для всех разбиений на всех
маркетах.

CalculateMarketEthReturns - формирует эфирный эквивалент, с учетом
газправса.

Потом рюкзак (см. доку - Dynamic Programming Parts)

На выходе мы получаем singleMarketParths - разбиение внутри блока.

** ComputeUniqRoots

Находим лучшей, находим к нему неконфликтующие маршруты, прикидываем все
маркеты, которые в этих маршрутах используются, пересчитываем все
остальные пути без этих маркетов, чтобы догрести еще какое-то кол-во
неконфликтующих путей. В итоге получаем обычно не больше пяти путей,
чтобы слить их в мультипуть

** Остаток

прогнать динамимический рюкзак но теперь между бранчами пути.

избегаем пересечения маркетов на путях


* Deploy

Я работаю в своей ветке. Чтобы запулить свои изменения на stage, надо
запушить ветку.

В корне есть 2 папки, ~k8s-new~ и ~k8s~, в них находятся уже
сгенерированные yaml-файлы нужные чтобы происходил деплой. Их надо менять
только из папки ~./scripts~ там есть папки ~k8s_new_yaml_generator~ и
~k8s_yaml_generator~ в них лежат сорцы питоновской кодогенерации,
которые берут за основу темплейт, который лежит в той же папке и
конфиг. Потому что слишком много чейнов и вручную это делать утомительно.

Там же лежит arbitrum_protocol_list.yaml в котором все протоколы, которые
включены. На основе этого генерируются worked deployment файлы и
поднимаются (или не поднимаются) воркеры. Сгенерированно скидывается в
~k8s-new~ и ~k8s~ а сам генерирующий скрипт - k8s_yaml_generator.py

В ~k8s-new~ и ~k8s~ есть файл tags.yaml - его надо изменить, чтобы он
ссылался на гитовские теги в моей ветке, которые должны быть
развернуты.

Там нельзя использовать слеши. Есть команда, которая берет
эти теги и вставляет во все деплойменты.

#+BEGIN_SRC sh
  alias uppfconf='cwd=$(pwd) && cd $INCH/scripts/k8s_new_yaml_generator/ && python3 k8s_yaml_generator.py && cd $INCH/scripts/k8s_yaml_generator/ && python3 k8s_yaml_generator.py && cd $cwd && unset cwd'
#+END_SRC

Потом этот таг мы определяем и пушим в репу

#+BEGIN_SRC sh
  git tag <sometag>
  git push --tags
#+END_SRC

Потом нужно сходить в github actions, в меню ~Build docker image~ и
убедиться что билд этого тега собрался.

Затем идем меню Workflows в ~ETH deploy staging~ его выбираем, потом
справа нажимаем ~Run workflow~, выбираем branch и раним. В ветке код
спулится, увидит какой указан тег, выберет этот тег и раскатит с тега.

Чтобы посмотреть логи в случае проблем нужно использовать тулзу kconf
https://github.com/particledecay/kconf/releases/tag/v1.9.0

Ее надо скачать для нужной операционки, внутри будет бинарник.

Sova скидывает два конфига, которые надо добавить через ~kconf add~

После добавления можно делать ~kconf use~ на один из них. Кластер
~hel~ маленький, там 120 машин, на ~fsn~ - 180.

kubectl если не установлен лучше качать бинарником с оффсайта.

kubectl и kconf лучше положить в ~/bin/ потому что он есть в $PATH

Проверить работают ли доступы можно с помощью:

#+BEGIN_SRC sh
  kubectl get pods -n shared
#+END_SRC

Также стоит смотреть в
https://github.com/1inch/infra-monitoring/blob/master/dev_guide.md
- там есть необходимые алиасы, которые стоит прописать в ~/.bashrc

После этого достаточно набрать ~graphana~ чтобы получить прокиную графану
на 3000 порт и зайти на нее через броузер. Логин и пароль к ней лежит в
https://github.com/1inch/infra-monitoring в REAME.md

На иконке квадратиков (третьей сверху) есть выпадающее меню с пунктом
~Manage~ - там есть дашборды для pathfinder-а.

Один из интересных - ~Pathfinder Details Staging~. Там можно смотреть
latency и прочее.

Есть также staging-апп: https://staging-app.1inch.io/#/1/swap/ETH/USDT
(лучше выбрать в менюшке trade->classic_mode). И запросы через этот
веб-интерфейс будут выполняться как раз на стейджинге, их можно увидеть в
метриках в графане. Из этого же интерфейса через консоль разработчика
можно выдрать нужные запросы.

Полезные команды:

ksn 1inch

kubectl -n 1inch get pods - выдает все поды этого кластера, можно
сделать | grep staging | grep pathfinder

Так можно получить нужные поды (не те, где redis в названии)

выбрать один, и в нем чтобы просмотреть логи

cubectl logs -n 1inch -f имя_пода

Чтобы зайти в под нужно сделать:

kubectl -n 1inch exec -it имя_пода /bin/sh

Еще одна полезная утилита - Lens -
https://github.com/lensapp/lens/releases/tag/v4.2.0 - нужен именно этот
релиз, его нельзя обновлять, иначе будут проблемы.

В нее после старта надо добавить кластеры (кнопка Add Clusters)

Потом можно идти в Workloads->Pods, дропдаун Namespace - выбрать 1inch и
в поле Searh можно искать pathfinder-что-то-там. Если по нему тыкнуть то
можно посмотреть некоторые метрики: CPU, Memory, Network,
Filesystem. Также справа вверху есть кнопочки с иконками, среди них есть
Logs - и так можно увидеть логи. Чтобы их же увидеть из консоли можно
сделать

#+BEGIN_SRC sh
  kgp | grep pathfinder-api-server-staging-v3-0
#+END_SRC

так можно получить хэш, вместе с ним скопировать и сделать

#+BEGIN_SRC sh
  kl pathfinder-api-server-staging-v3-0-797b4bcbfc-lmjcf
#+END_SRC

чтобы получить логи.

Чтобы что-то подебажить можно найти контейнер ubuntu-test, в lens можно
получить его консоль. Из нее можно обратиться к любому pathfinderu
используя curl и взяв аргумент с фронта.

Еще внутри куба можно обращаться от пода к поду по его доменному имени,
которое совпадает с названием деплоймента. Так что можно смодифицировать
наш запрос в любом редакторе (заменив https на http и порт на 8088)

Мы можем прокинуть себе терминал:

#+BEGIN_SRC sh
  k exec -it -n 1inch ubuntu-test /bin/bash
#+END_SRC

и туда кидать этот модифицированный запрос.

Еще можно заинсталлить на этот под ~wrk~ и делать им нагрузку.

#+BEGIN_SRC sh
  wrk -c 10 -t 10 'сам_курловый_запрос'
#+END_SRC

и потом идти в графану смотреть метрики на pathfinder-staging
(latency и остальные метрики)
